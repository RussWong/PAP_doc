# 特征选择

特征选择是对输入数据的特征进行筛选，剔除一些重要性较低的特征，降低模型训练复杂度并提高模型性能的一种特征处理方法。

包括以下两部分：

* 特征排序
* 特征删除



## 特征排序

选择了sklearn中适合回归模型的六种特征选择方式，对特征进行排序，每个模型返回一组特征重要性排序。六种特征选择方式分别为：

* f_regression:以方差分析为基础，对特征进行排序
* mutual_info_regression:估计目标变量的互信息。两个随机变量之间的互信息是非负的，它度量了变量之间的相关性。当且仅当两个随机变量相互独立时，它等于零，且越高的值意味着越高的相依性
* RFE+LR:RFE使用作为外部模型
* RFE+LassoCV:RFE使用Lasso回归作为外部模型
* RFE+RidgeCV:RFE使用岭回归作为外部模型
* RFE+RFR:RFE使用随机森林作为外部模型

注：RFE又名递归特征消除，它使用一个外部估计器来分配特征的权重，然后递归删除最不重要的特征，以此获得越来越少的特征集。通过输入不同的模型给RFE作为权重选择器，衍生出不同的特征选择方案

## 特征删除

通过6种特征选择算法获得了6组特征排序。通过采取投票法判断特征是否要保留，假设要保留的最终特征数为N。则6个排序分别对每个特征给出结果：0（该特征排序数不在前N位）或1（该特征排序数在前N位）。最后根据每个特征获得的票数进行排序，保留最终前N个特征（票数相同看分排名总和，分排名之和靠前的优先）

## 说明

特征选择模块封装在FeatureSelection模块中，位于数据预处理part1、异常检测part2、特征编码part3之后，是第四个模块，除了之前模块所必须的参数之外，该模型有一个自己的额外参数num_of_feature，即最终保留的特征数，默认值为15。